Enhanced Knowledge Distillation for Image Classification (Hypergraph-ViT Teacher, ResNet Student)

Table of Contents
- [Introduction](#introduction)
- [Project Overview](#project-overview)
- [Features](#features)
- [Dataset](#dataset)
- [Installation](#installation)
- [Usage](#usage)
- [Results](#results)
- [Model Architectures](#model-architectures)
- [Knowledge Distillation Loss](#knowledge-distillation-loss)

Introduction

This repository presents an implementation of Knowledge Distillation for image classification, featuring an innovative Hypergraph-Enhanced Vision Transformer (ViT) as the teacher model. The goal is to transfer the enriched knowledge from this advanced teacher to a smaller, more efficient ResNet50 student model. This approach aims to leverage the representational power of hypergraphs to improve the teacher's understanding of complex relationships within image patches, thereby enabling the student model to achieve superior performance with a reduced computational footprint.

This project was developed as a final year project focusing on advanced deep learning techniques and novel architectural enhancements for practical applications in computer vision.

Project Overview

The core of this project involves:

 * Loading pre-trained Vision Transformer (DeiT Small) as the teacher model.

 * Augmenting the Vision Transformer teacher with a custom HypergraphConv layer to process image patch tokens.

 * Fusing the hypergraph-enhanced features with the ViT's classification token for a more informative teacher signal.

 * Using a pre-trained ResNet50 as the student model.

 * Adapting both models for a 2-class image classification task (classes 'B' and 'M').

 * Implementing a custom Knowledge Distillation loss function that combines the Kullback-Leibler (KL) divergence between teacher and student logits with the standard Cross-Entropy loss.

 * Training the student model using the distilled knowledge from the teacher, along with traditional supervision.

Features

 * Hypergraph-Enhanced Teacher: Integration of custom HypergraphConv layer into the Vision Transformer teacher model for richer feature representation.
 * Knowledge Distillation: Effective transfer of knowledge from a powerful, enhanced teacher model to a compact student model.
 * Pre-trained Models: Utilizes pre-trained deit_small_distilled_patch16_224 (ViT) as the base for the teacher and ResNet50 as the student.
 * Data Augmentation: Includes RandomHorizontalFlip, RandomRotation, and ColorJitter for robust model training.
 * Custom Loss Function: Implements a combined KD and Cross-Entropy loss for optimized training.
 * Learning Rate Scheduling: Uses StepLR to adjust the learning rate during training for better convergence.
 * Performance Visualization: Plots training and validation loss and accuracy curves, and generates a confusion matrix and classification report.
 * GPU Acceleration: Configured to utilize CUDA if available for faster training.

Hypergraph Integration

 * The project incorporates Hypergraph Convolution to enhance the Vision Transformer's ability to capture complex, non-pairwise relationships within the input image patches.

HypergraphConv Layer
 
 This is a custom PyTorch module that implements hypergraph convolution. It operates on the patch tokens generated by the ViT:

   * It computes a similarity matrix S between normalized patch tokens.

   * An incidence matrix H is constructed by thresholding this similarity matrix, effectively defining hyperedges based on strong correlations between patches.

   * Node and hyperedge degree matrices (Dv, De) are calculated to normalize the message passing.

   * The layer then performs an aggregation process akin to graph convolution, but extended to hypergraphs, allowing information to flow through groups of related patches (hyperedges).

   * Finally, a linear transformation and ReLU activation are applied.

Integration into TModel (Teacher)

 The HypergraphConv layer is integrated directly into the TModel architecture:

   1. Patch Token Extraction: The Vision Transformer's patch_embed and transformer blocks process the input image to extract rich patch tokens.

   2. Hypergraph Processing: These patch tokens are then fed into the HypergraphConv layer, which generates hyper_features representing the hypergraph-aware information.

   3. Feature Fusion: The average of these hyper_features is concatenated with the Vision Transformer's learned cls_token (classification token).

   4. Final Prediction: This combined, fused feature vector is passed through a fusion layer and then a classifier to produce the teacher's enhanced logits.

This design allows the teacher model to leverage both the global context learned by the ViT and the local, higher-order relationships captured by the hypergraph, providing a more comprehensive and robust source of knowledge for distillation.

Dataset

* Dataset Path: `/kaggle/input/enhanced-images/Attention_Dataset`
* Classes: 'B' (index 0) and 'M' (index 1).
* Preprocessing: Images are resized to 224x224 and normalized.

 Installation

To set up the project environment, follow these steps:

1.  Clone the repository:
    ```bash
    git clone [https://github.com/YourUsername/Knowledge-Distillation-ViT-ResNet-Image-Classification.git](https://github.com/YourUsername/Knowledge-Distillation-ViT-ResNet-Image-Classification.git)
    cd Knowledge-Distillation-ViT-ResNet-Image-Classification
    ```
2.  Create a virtual environment (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: `venv\Scripts\activate`
    ```
3.  Install dependencies:
    ```bash
    pip install torch torchvision timm matplotlib torchsummary
    ```

Usage

To run the knowledge distillation training and evaluation, execute the Jupyter Notebook:

1. Ensure your dataset is correctly placed at ` /kaggle/input/enhanced-images/Attention_Dataset` or update the `dataset_path` variable in the notebook to your dataset's location.
2. Open the Jupyter Notebook:
    ```bash
    jupyter notebook Final_Year_Project.ipynb
    ```
3.  Run all cells in the notebook. The script will:
    * Load and preprocess the dataset.
    * Initialize and adapt the teacher and student models.
    * Perform knowledge distillation training over 20 epochs.
    * Display training and validation metrics for each epoch.
    * Generate plots for student model's training/validation loss and accuracy.

Results

After 20 epochs of training, the student model achieved the following performance metrics:

* Final Training Loss: ~0.1813
* Final Training Accuracy:~0.9724
* Final Validation Loss: ~0.1793
* Final Validation Accuracy:~0.9729

Model Architectures

Teacher Model: Vision Transformer (ViT)
* Base Model: deit_small_distilled_patch16_224 (ViT) from timm
* Enhancement: Integrates a HypergraphConv layer after the ViT's transformer blocks to process patch tokens.
* Fusion: Fuses the hypergraph-enhanced features with the ViT's classification token.
* Key Modification: Output head adapted for 2 classification classes.

Student Model: ResNet50
* Base Model: resnet50 from torchvision.models
* Key Modification: Fully connected layer replaced with a nn.Sequential block including Dropout (0.5) and a Linear layer for 2 classification classes.

Knowledge Distillation Loss

The knowledge distillation loss function used in this project is a weighted sum of two components:

1.  Kullback-Leibler (KL) Divergence: Measures the difference between the softened probabilities of the teacher and student models. A higher temperature `T` (set to 5) is used to soften the logits, providing more information about the teacher's "dark knowledge".
2.  Cross-Entropy Loss: The standard supervised loss calculated between the student's predictions and the true ground-truth labels.
The combined loss is calculated as:
```python
loss = alpha * kd_loss + (1 - alpha) * ce_loss
